{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Step 1:- Import the required libraries \n\n\nimport numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:13.044332Z","iopub.execute_input":"2022-05-08T13:31:13.044672Z","iopub.status.idle":"2022-05-08T13:31:18.117515Z","shell.execute_reply.started":"2022-05-08T13:31:13.044593Z","shell.execute_reply":"2022-05-08T13:31:18.116781Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"## Step 2 : Data Loading and Pre-Processing\n\ntoken_path = \"../input/flickr-8k/Flickr8k.token.txt\"\ntrain_images_path = '../input/flickr-8k/Flickr_8k.trainImages.txt'\ntest_images_path = '../input/flickr-8k/Flickr_8k.testImages.txt'\nimages_path = '../input/flickr8k/Images/'\nglove_path = '../input/glove6b/'\n\ndoc = open(token_path,'r').read()\nprint(doc[:410])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.119355Z","iopub.execute_input":"2022-05-08T13:31:18.119600Z","iopub.status.idle":"2022-05-08T13:31:18.156086Z","shell.execute_reply.started":"2022-05-08T13:31:18.119566Z","shell.execute_reply":"2022-05-08T13:31:18.155428Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**So we can see the format in which our image id’s and their captions are stored.\nNext, we create a dictionary named “descriptions” which contains the name of the image as keys and a list of the 5 captions for the corresponding image as values.**","metadata":{}},{"cell_type":"code","source":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.157349Z","iopub.execute_input":"2022-05-08T13:31:18.157583Z","iopub.status.idle":"2022-05-08T13:31:18.290860Z","shell.execute_reply.started":"2022-05-08T13:31:18.157549Z","shell.execute_reply":"2022-05-08T13:31:18.290032Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Now let’s perform some basic text clean to get rid of punctuation and convert our descriptions to lowercase.**","metadata":{}},{"cell_type":"code","source":"table = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.295877Z","iopub.execute_input":"2022-05-08T13:31:18.301831Z","iopub.status.idle":"2022-05-08T13:31:18.656015Z","shell.execute_reply.started":"2022-05-08T13:31:18.301784Z","shell.execute_reply":"2022-05-08T13:31:18.655266Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## Lets visualize and example image\npic = '1000268201_693b08cb0e.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.658203Z","iopub.execute_input":"2022-05-08T13:31:18.658461Z","iopub.status.idle":"2022-05-08T13:31:18.895148Z","shell.execute_reply.started":"2022-05-08T13:31:18.658426Z","shell.execute_reply":"2022-05-08T13:31:18.894462Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Next, we create a vocabulary of unique words present across all the 8000 * 5(40000) image captions in the dataset.**\n\n**We have 8828 unique words across all the 40000 image captions.**","metadata":{}},{"cell_type":"code","source":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\nprint('Original Vocabulary Size: %d' % len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.896097Z","iopub.execute_input":"2022-05-08T13:31:18.896893Z","iopub.status.idle":"2022-05-08T13:31:18.962479Z","shell.execute_reply.started":"2022-05-08T13:31:18.896857Z","shell.execute_reply":"2022-05-08T13:31:18.961702Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Now let’s save the image id’s and their new cleaned captions in the same format as the token.txt file**","metadata":{}},{"cell_type":"code","source":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.963793Z","iopub.execute_input":"2022-05-08T13:31:18.964246Z","iopub.status.idle":"2022-05-08T13:31:18.987286Z","shell.execute_reply.started":"2022-05-08T13:31:18.964209Z","shell.execute_reply":"2022-05-08T13:31:18.986557Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Next, we load all the 6000 training image_id's in a variable train**","metadata":{}},{"cell_type":"code","source":"doc = open(train_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntrain = set(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:18.988813Z","iopub.execute_input":"2022-05-08T13:31:18.989241Z","iopub.status.idle":"2022-05-08T13:31:19.006982Z","shell.execute_reply.started":"2022-05-08T13:31:18.989177Z","shell.execute_reply":"2022-05-08T13:31:19.006279Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Now we save all the training and testing images in train_img and test_img lists respectively:**","metadata":{}},{"cell_type":"code","source":"img = glob.glob(images_path + '*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i[len(images_path):] in train_images:\n        train_img.append(i)\n\ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i[len(images_path):] in test_images: \n        test_img.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.008156Z","iopub.execute_input":"2022-05-08T13:31:19.008772Z","iopub.status.idle":"2022-05-08T13:31:19.371101Z","shell.execute_reply.started":"2022-05-08T13:31:19.008732Z","shell.execute_reply":"2022-05-08T13:31:19.370247Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Now, we will load the descriptions of the training images into a dictionary. However, we will add two tokens in every capation, which are 'startseq' and 'endseq'.**\n","metadata":{}},{"cell_type":"code","source":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.375819Z","iopub.execute_input":"2022-05-08T13:31:19.378104Z","iopub.status.idle":"2022-05-08T13:31:19.513164Z","shell.execute_reply.started":"2022-05-08T13:31:19.376531Z","shell.execute_reply":"2022-05-08T13:31:19.512407Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Crate a list of all the training captions**","metadata":{}},{"cell_type":"code","source":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.514642Z","iopub.execute_input":"2022-05-08T13:31:19.515136Z","iopub.status.idle":"2022-05-08T13:31:19.527911Z","shell.execute_reply.started":"2022-05-08T13:31:19.515080Z","shell.execute_reply":"2022-05-08T13:31:19.527050Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**To make our model more robust, we will reduce our vocabulary to only those words which occur atleast 10 times in the entire corpus**","metadata":{}},{"cell_type":"code","source":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint('Vocabulary = %d' % (len(vocab)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.529716Z","iopub.execute_input":"2022-05-08T13:31:19.529948Z","iopub.status.idle":"2022-05-08T13:31:19.774352Z","shell.execute_reply.started":"2022-05-08T13:31:19.529916Z","shell.execute_reply":"2022-05-08T13:31:19.773138Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Now, we will create two dictionaries to map words to an index and vice versa. Also, we will append 1 to our vocabulary since we append 0 to make all the captions of equal lengths**","metadata":{}},{"cell_type":"code","source":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.776030Z","iopub.execute_input":"2022-05-08T13:31:19.776286Z","iopub.status.idle":"2022-05-08T13:31:19.785160Z","shell.execute_reply.started":"2022-05-08T13:31:19.776252Z","shell.execute_reply":"2022-05-08T13:31:19.784513Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Hence we now have a vocab size of 1660.**\n\n**We also need to find out what maximum length of a caption can be since we can't have have captions of arbitary length.**","metadata":{}},{"cell_type":"code","source":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.790956Z","iopub.execute_input":"2022-05-08T13:31:19.791736Z","iopub.status.idle":"2022-05-08T13:31:19.848766Z","shell.execute_reply.started":"2022-05-08T13:31:19.791699Z","shell.execute_reply":"2022-05-08T13:31:19.848161Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Glove Embeddings\n\n**Word vectors map words to a vector space, where similar words are clustered together and different words are separated.\nThe basic premise behind Glove is that we can derive sematic relationships between words from the coouucrence matrix.**\n\n**For our model, we will map all the 38-word long caption to a 200-dimension vector using Glove.**","metadata":{}},{"cell_type":"code","source":"embeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:19.852234Z","iopub.execute_input":"2022-05-08T13:31:19.852759Z","iopub.status.idle":"2022-05-08T13:31:42.028748Z","shell.execute_reply.started":"2022-05-08T13:31:19.852722Z","shell.execute_reply":"2022-05-08T13:31:42.028034Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Now, we make metrix of shape(1660,220) consisting of our vocabulary and the 200-d vector.**","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:42.030765Z","iopub.execute_input":"2022-05-08T13:31:42.031305Z","iopub.status.idle":"2022-05-08T13:31:42.042151Z","shell.execute_reply.started":"2022-05-08T13:31:42.031264Z","shell.execute_reply":"2022-05-08T13:31:42.041432Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Step 4 : Model building and Training\n\n**A approach we have adopted is transfer learning using Inception v-3 network which is pre-trained on the Image-Net dataset.**","metadata":{}},{"cell_type":"code","source":"model = InceptionV3(weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:42.043757Z","iopub.execute_input":"2022-05-08T13:31:42.044178Z","iopub.status.idle":"2022-05-08T13:31:47.347222Z","shell.execute_reply.started":"2022-05-08T13:31:42.044140Z","shell.execute_reply":"2022-05-08T13:31:47.346344Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_new = Model(model.input, model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:47.348651Z","iopub.execute_input":"2022-05-08T13:31:47.348934Z","iopub.status.idle":"2022-05-08T13:31:47.376321Z","shell.execute_reply.started":"2022-05-08T13:31:47.348895Z","shell.execute_reply":"2022-05-08T13:31:47.375322Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Since we are using Inception V-3 , we nedd to pre-process our inputs before feeding it into the model.\nHence, we need to defince a preprocess function to reshape the images(299 x 299) and feed the preprocess_input() function.**","metadata":{}},{"cell_type":"code","source":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:47.377672Z","iopub.execute_input":"2022-05-08T13:31:47.378099Z","iopub.status.idle":"2022-05-08T13:31:47.383813Z","shell.execute_reply.started":"2022-05-08T13:31:47.378054Z","shell.execute_reply":"2022-05-08T13:31:47.382785Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Now, we can go ahead with training and testing images i.e we can extract the images vectors of shape (2048 ,)**","metadata":{}},{"cell_type":"code","source":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    return fea_vec\n\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images_path):]] = encode(img)\ntrain_features = encoding_train\n\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images_path):]] = encode(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:31:47.386328Z","iopub.execute_input":"2022-05-08T13:31:47.386837Z","iopub.status.idle":"2022-05-08T13:39:02.018789Z","shell.execute_reply.started":"2022-05-08T13:31:47.386796Z","shell.execute_reply":"2022-05-08T13:39:02.018034Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Now let’s define our model.**\n\n**We are creating a Merge model where we combine the image vector and the partial caption. Therefore our model will have 3 major steps:**\n\n**Processing the sequence from the text**\n\n**Extracting the feature vector from the image**\n\n**Decoding the output using softmax by concatenating the above two layers**","metadata":{}},{"cell_type":"code","source":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:39:02.020289Z","iopub.execute_input":"2022-05-08T13:39:02.020552Z","iopub.status.idle":"2022-05-08T13:39:03.056721Z","shell.execute_reply.started":"2022-05-08T13:39:02.020516Z","shell.execute_reply":"2022-05-08T13:39:03.056000Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Input_3 is the partial caption of max length 34 which is fed into the embedding layer. This is where the words are mapped to the 200-d Glove embedding. It is followed by a dropout of 0.5 to avoid overfitting. This is then fed into the LSTM for processing the sequence.**\n\n**Input_2 is the image vector extracted by our InceptionV3 network. It is followed by a dropout of 0.5 to avoid overfitting and then fed into a Fully Connected layer.**\n\n**Both the Image model and the Language model are then concatenated by adding and fed into another Fully Connected layer. The layer is a softmax layer that provides probabilities to our 1660 word vocabulary.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Step 5 : Model Training\n\n**Before training the model we need to keep in mind that we do not want to retrain the weights in our embedding layer (pre-trained Glove vectors).**","metadata":{}},{"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:39:03.057980Z","iopub.execute_input":"2022-05-08T13:39:03.058235Z","iopub.status.idle":"2022-05-08T13:39:03.064166Z","shell.execute_reply.started":"2022-05-08T13:39:03.058200Z","shell.execute_reply":"2022-05-08T13:39:03.063135Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Next, compile the model using Categorical_Crossentropy as the Loss function and Adam as the optimizer.**\n","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:39:03.065550Z","iopub.execute_input":"2022-05-08T13:39:03.067259Z","iopub.status.idle":"2022-05-08T13:39:03.082128Z","shell.execute_reply.started":"2022-05-08T13:39:03.067228Z","shell.execute_reply":"2022-05-08T13:39:03.081481Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Since our dataset has 6000 images and 40000 captions we will create a function that can train the data in batches.**\n","metadata":{}},{"cell_type":"code","source":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key +'.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:39:03.083207Z","iopub.execute_input":"2022-05-08T13:39:03.083526Z","iopub.status.idle":"2022-05-08T13:39:03.094724Z","shell.execute_reply.started":"2022-05-08T13:39:03.083492Z","shell.execute_reply":"2022-05-08T13:39:03.093944Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Now , let's train our model with for 30 epochs with a batch_size of 3 and 2000 steps per epoch.**","metadata":{}},{"cell_type":"code","source":"epochs = 5\nbatch_size = 3\nsteps = len(train_descriptions)//batch_size\n\ngenerator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:39:03.095765Z","iopub.execute_input":"2022-05-08T13:39:03.096327Z","iopub.status.idle":"2022-05-08T13:56:55.647699Z","shell.execute_reply.started":"2022-05-08T13:39:03.096161Z","shell.execute_reply":"2022-05-08T13:56:55.646985Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Step 6: Greddy and Beam Search**\n\n**As the model generates a 1660 long vector with a probability distribution across all the words in the vocabulary we greedily pick the word with the highest probability to get the next word prediction. This method is called Greedy Search.**\n\n","metadata":{}},{"cell_type":"code","source":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:56:55.648905Z","iopub.execute_input":"2022-05-08T13:56:55.650265Z","iopub.status.idle":"2022-05-08T13:56:55.657128Z","shell.execute_reply.started":"2022-05-08T13:56:55.650225Z","shell.execute_reply":"2022-05-08T13:56:55.656267Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Beam Search is where we take top k predictions, feed them again in the model and then sort them using the probabilities returned by the model. So, the list will always contain the top k predictions and we take the one with the highest probability and go through it till we encounter ‘endseq’ or reach the maximum caption length.**\n","metadata":{}},{"cell_type":"code","source":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    #print(final_caption)\n    final_caption = ' '.join(final_caption[1:])\n    \n    return final_caption","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:30:13.034438Z","iopub.execute_input":"2022-05-08T16:30:13.034977Z","iopub.status.idle":"2022-05-08T16:30:13.044189Z","shell.execute_reply.started":"2022-05-08T16:30:13.034940Z","shell.execute_reply":"2022-05-08T16:30:13.043412Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"**Step 7:- Evaluation**\n\n\n**Let’s now test our model on different images and see what captions it generates. We will also look at the different captions generated by Greedy search and Beam search with different k values.**\n\n**First, we will take a look at the example image we saw at the start of the article. We saw that the caption for the image was ‘A black dog and a brown dog in the snow’. Let’s see how our model compares**","metadata":{}},{"cell_type":"markdown","source":"Function for calculating bleu score.","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import corpus_bleu\ndef calc_bleu_score(prediction, actuals):\n    scores = []\n    for actual in actuals:\n        scores.append(sentence_bleu(prediction.split(), actual.split()))\n    \n    return np.max(scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:30:14.243276Z","iopub.execute_input":"2022-05-08T16:30:14.243810Z","iopub.status.idle":"2022-05-08T16:30:14.249224Z","shell.execute_reply.started":"2022-05-08T16:30:14.243774Z","shell.execute_reply":"2022-05-08T16:30:14.247986Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-1","metadata":{}},{"cell_type":"code","source":"pic = '2398605966_1d0c9e6a20.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['2398605966_1d0c9e6a20']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:07:00.029169Z","iopub.execute_input":"2022-05-08T17:07:00.029699Z","iopub.status.idle":"2022-05-08T17:07:20.998595Z","shell.execute_reply.started":"2022-05-08T17:07:00.029665Z","shell.execute_reply":"2022-05-08T17:07:20.997816Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-2","metadata":{}},{"cell_type":"code","source":"pic = '2511019188_ca71775f2d.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['2511019188_ca71775f2d']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:07:21.000687Z","iopub.execute_input":"2022-05-08T17:07:21.001142Z","iopub.status.idle":"2022-05-08T17:07:41.484489Z","shell.execute_reply.started":"2022-05-08T17:07:21.001084Z","shell.execute_reply":"2022-05-08T17:07:41.483781Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-3","metadata":{}},{"cell_type":"code","source":"pic = '493109089_468e105233.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['493109089_468e105233']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:07:41.485809Z","iopub.execute_input":"2022-05-08T17:07:41.486055Z","iopub.status.idle":"2022-05-08T17:08:02.062219Z","shell.execute_reply.started":"2022-05-08T17:07:41.486020Z","shell.execute_reply":"2022-05-08T17:08:02.061463Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-4","metadata":{}},{"cell_type":"code","source":"pic = '2346401538_f5e8da66fc.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['2346401538_f5e8da66fc']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:08:02.064040Z","iopub.execute_input":"2022-05-08T17:08:02.064770Z","iopub.status.idle":"2022-05-08T17:08:22.839186Z","shell.execute_reply.started":"2022-05-08T17:08:02.064732Z","shell.execute_reply":"2022-05-08T17:08:22.837425Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-5","metadata":{}},{"cell_type":"code","source":"pic = '3694991841_141804da1f.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['3694991841_141804da1f']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:08:22.840676Z","iopub.execute_input":"2022-05-08T17:08:22.840943Z","iopub.status.idle":"2022-05-08T17:08:43.730186Z","shell.execute_reply.started":"2022-05-08T17:08:22.840912Z","shell.execute_reply":"2022-05-08T17:08:43.728750Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Prediction for image-6","metadata":{}},{"cell_type":"code","source":"pic = '1773928579_5664a810dc.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\ndescr = descriptions['1773928579_5664a810dc']\nprint('Actual captions of image: ',descr)\n\n# Greedy Search\ngd = greedySearch(image)\nprint(\"***** Greedy Search *****\")\nprint(\"Predicted sentence:\",gd)\nprint(\"Bleu score: \", calc_bleu_score(gd,descr))\n\n# Beam Search, K = 3\nb3 = beam_search_predictions(image, beam_index = 3)\nprint(\"***** Beam Search K = 3 *****\")\nprint(\"Predicted sentence:\",b3)\nprint(\"Bleu score: \", calc_bleu_score(b3,descr))\n\n# Beam Search, K = 10\nb10 = beam_search_predictions(image, beam_index = 10)\nprint(\"***** Beam Search K = 10 *****\")\nprint(\"Predicted sentence:\",b10)\nprint(\"Bleu score: \", calc_bleu_score(b10,descr))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:08:43.731422Z","iopub.execute_input":"2022-05-08T17:08:43.731672Z","iopub.status.idle":"2022-05-08T17:09:05.189738Z","shell.execute_reply.started":"2022-05-08T17:08:43.731637Z","shell.execute_reply":"2022-05-08T17:09:05.187960Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}